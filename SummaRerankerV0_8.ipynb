{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkAZpLqNaYiQeArReRitiA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CakeVision/collabs/blob/main/SummaRerankerV0_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MVd9OWkL8Yw",
        "outputId": "d371afa4-ed98-4f65-f0b9-62ab839fea1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqHWpZ_tRBPu"
      },
      "outputs": [],
      "source": [
        "# most of the code belongs to https://github.com/ntunlp/SummaReranker. (arxiv paper:2203.06569)\n",
        "## i had to make changes so it would run better in the collab enviorment(modify some utils and some class components)\n",
        "\n",
        "import tqdm\n",
        "import pickle\n",
        "import os\n",
        "from shutil import copyfile\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "import transformers\n",
        "from transformers import PegasusTokenizer,PegasusForConditionalGeneration,RobertaTokenizerFast, RobertaTokenizer, RobertaModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SET Data HyperParams(fancy name for things set by the dev)\n",
        "data_folder = \"/content/root/\"\n",
        "data_threshhold = 143000\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything()"
      ],
      "metadata": {
        "id": "_VqagVpoGQAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_builder():\n",
        "  train_summaries, train_texts = [], []\n",
        "  with open(data_folder + \"train_summary.txt\", \"rb\") as f:\n",
        "    for l in f.readlines():\n",
        "        train_summaries.append(l)\n",
        "  with open(data_folder + \"train_text.txt\", \"rb\") as f:\n",
        "    for l in f.readlines():\n",
        "      train_texts.append(l)\n",
        "\n",
        "  perm = np.random.permutation(len(train_texts))\n",
        "  with open(data_folder + \"training_permutation.pkl\", \"wb\") as f:\n",
        "    pickle.dump(p, f)\n",
        "  train_summaries = [train_summaries[i] for i in p]\n",
        "  train_texts = [train_texts[i] for i in p]\n",
        "  # reverse permutations\n",
        "  for i in range(len(perm)):\n",
        "    reverse_perm[perm[i]] = i\n",
        "\n",
        "  # separation of dataset into 2 halves (mostly for computation efficiency and storage),\n",
        "  # most of the delay in my understanding is caused by file size\n",
        "\n",
        "  #first half\n",
        "  first_half_summaries = train_summaries[:data_threshhold]\n",
        "  first_half_texts = train_texts[:data_threshhold]\n",
        "\n",
        "  with open(data_folder + \"fh_shuffled_summary.txt\", \"wb\"):\n",
        "    for l in first_half_summaries:\n",
        "      f.write(l)\n",
        "\n",
        "  with open(data_folder + \"fh_shuffled_text.txt\", \"wb\"):\n",
        "    for l in first_half_summaries:\n",
        "      f.write(l)\n",
        "\n",
        "  #second half(copy paste first)\n",
        "  second_half_summaries = train_summaries[:data_threshhold]\n",
        "  second_half_texts = train_texts[:data_threshhold]\n",
        "\n",
        "  with open(data_folder + \"sh_shuffled_summary.txt\", \"wb\"):\n",
        "    for l in second_half_summaries:\n",
        "      f.write(l)\n",
        "\n",
        "  with open(data_folder + \"sh_shuffled_text.txt\", \"wb\"):\n",
        "    for l in second_half_summaries:\n",
        "      f.write(l)\n",
        "\n",
        "  ftypes = [\"summary\", \"text\"]\n",
        "  for ftype in ftypes:\n",
        "    path = data_folder + \"train/{}/\".format(ftype)\n",
        "    idx_first = 0\n",
        "    idx_second = 0\n",
        "    for i in tqdm(range(len(p))):\n",
        "      cur_path = path + \"train_{}_{}.txt\".format(doc, perm[i])\n",
        "      if i < data_threshhold:\n",
        "        nf_path = data_folder + \"fh_train_shuffled/{}/shuffled_train_{}_{}.txt\".format(ftype, ftype, idx_first)\n",
        "        idx_first += 1\n",
        "      else:\n",
        "        nf_path = data_folder + \"sh_train_shuffled/{}/shuffled_train_{}_{}.txt\".format(ftype, ftype, idx_first)\n",
        "        idx_first += 1\n",
        "      copyfile(cur_path, nf_path)"
      ],
      "metadata": {
        "id": "2PVYvN619yJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Base Data UTILS\n",
        "\n",
        "def load_data(set,data_folder, individual_txt = False):\n",
        "  if individual_txt:\n",
        "    texts, summaries = read_data_files_individual(set, data_folder)\n",
        "  else:\n",
        "    text_files, summary_files = prepare_data_files(set, data_folder)\n",
        "    texts, summaries = read_data_files(set, text_files, summary_files, data_folder)\n",
        "\n",
        "  print(\"Total # of texts: {}, # summaries: {}\".format(len(texts), len(summaries)))\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(texts[0])\n",
        "  print(summaries[0])\n",
        "\n",
        "  return texts, summaries\n",
        "\n",
        "def read_data_files_individual(set, data_folder):\n",
        "    texts = []\n",
        "    summaries = []\n",
        "    set_text_path = data_folder + \"/\" + set + \"/\" + \"text/\"\n",
        "    set_summary_path = data_folder + \"/\" + set + \"/\" + \"summary/\"\n",
        "    n_docs = len(os.listdir(set_text_path))\n",
        "    print(\"There are {} {} documents\".format(n_docs, set))\n",
        "    for i in range(n_docs):\n",
        "      text_path_i = set_text_path + \"{}_text_{}.txt\".format(set, i)\n",
        "      text_i = \"\".join(open(text_path_i, \"r\").readlines())\n",
        "      texts.append(text_i)\n",
        "    for i in range(n_docs):\n",
        "      summary_path_i = set_summary_path + \"{}_summary_{}.txt\".format(set, i)\n",
        "      summary_i = \"\".join(open(summary_path_i, \"r\").readlines())\n",
        "      summaries.append(summary_i)\n",
        "\n",
        "    return texts, summaries\n",
        "\n",
        "def read_data_files(set, text_files, summary_files, args):\n",
        "    # read the .txt files\n",
        "    texts = []\n",
        "    summaries = []\n",
        "    idx = 0\n",
        "    for text_file in text_files:\n",
        "        with open(text_file, 'r') as f:\n",
        "            lines = []\n",
        "            for l in tqdm(f.readlines()):\n",
        "                lines.append(l)\n",
        "            print(\"# lines: {}\".format(len(lines)))\n",
        "            texts += lines\n",
        "\n",
        "    for summary_file in summary_files:\n",
        "        with open(summary_file, 'r') as f:\n",
        "            lines = []\n",
        "            for l in tqdm(f.readlines()):\n",
        "                lines.append(l)\n",
        "            print(\"# lines: {}\".format(len(lines)))\n",
        "            summaries += lines\n",
        "\n",
        "    return texts, summaries\n",
        "\n",
        "def prepare_data_files(set, data_folder):\n",
        "    # find the files\n",
        "    text_files = []\n",
        "    summary_files = []\n",
        "    text_file = data_folder + \"/{}_text.txt\".format(set)\n",
        "    text_files.append(text_file)\n",
        "    summary_file = data_folder + \"/{}_summary.txt\".format(set)\n",
        "    summary_files.append(summary_file)\n",
        "    print(text_files)\n",
        "    print(summary_files)\n",
        "\n",
        "    return text_files, summary_files\n"
      ],
      "metadata": {
        "id": "ycgr9Ts6GRwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA UTILS WITH FOR SCORES SUMMARIES\n",
        "\n",
        "#TODO: construct other datasets and add them as a training option\n",
        "\n",
        "#rewrite load_data\n",
        "def load_data(set, size, args, individual_txt=True, train=False):\n",
        "    # texts & summaries\n",
        "    \"was easier to just add the individual text module, rather than remake it(the split to individual files wasn't the initial plan for the project)\"\n",
        "    \"args - dataset, scoring_method, n_beams\"\n",
        "    \"args.dataset is just for creating good file paths(can be replaced with a string)\"\n",
        "    \"would've been used for the multiple datasets mentioned in the paper\"\n",
        "    if individual_txt:\n",
        "        texts, summaries = read_data_files_individual(set, args, train=train)\n",
        "    else:\n",
        "        text_files, summary_files = prepare_data_files(set, args, train=train)\n",
        "        texts, summaries = read_data_files(text_files, summary_files, args)\n",
        "\n",
        "    # scored summaries - with multiple scores\n",
        "    if train:\n",
        "        all_scored_summaries = []\n",
        "        for generation_method in args.generation_methods:\n",
        "            gen_scored_summaries = []\n",
        "            for j in range(len(args.scoring_methods)):\n",
        "                scored_summaries_j = []\n",
        "                for i in range(len(set)):\n",
        "                    set_ = set[i]\n",
        "                    size_ = size[i]\n",
        "                    model_name_ = args.train_model_names[i]\n",
        "                    print(set_)\n",
        "                    print(size_)\n",
        "                    print(model_name_)\n",
        "                    scored_summaries_path_j_i = \"../../scored_summaries/{}/{}/{}/{}/{}_scored_summaries_{}_{}_beams_{}.pkl\".format(\n",
        "                        args.dataset, set_, generation_method, args.scoring_methods[j],\n",
        "                        set_, model_name_, size_, args.n_beams\n",
        "                    )\n",
        "                    print(scored_summaries_path_j_i)\n",
        "                    with open(scored_summaries_path_j_i, \"rb\") as f:\n",
        "                        scored_summaries_j_i = pickle.load(f)\n",
        "                    scored_summaries_j += scored_summaries_j_i\n",
        "                gen_scored_summaries.append(scored_summaries_j)\n",
        "            scored_summaries = []\n",
        "            for i in range(len(gen_scored_summaries[0])):\n",
        "                summaries_i = gen_scored_summaries[0][i][0]\n",
        "                scores_i = []\n",
        "                for j in range(len(args.scoring_methods)):\n",
        "                    scores_i_j = gen_scored_summaries[j][i][1]\n",
        "                    scores_i.append(scores_i_j)\n",
        "                scored_summaries.append((summaries_i, scores_i))\n",
        "            print(len(scored_summaries), len(scored_summaries[0]), len(scored_summaries[0][0]), len(scored_summaries[0][1]), len(scored_summaries[0][1][0]))\n",
        "            all_scored_summaries.append(scored_summaries)\n",
        "        scored_summaries = combine_summaries(all_scored_summaries)\n",
        "    else:\n",
        "        all_scored_summaries = []\n",
        "        for generation_method in args.generation_methods:\n",
        "            gen_scored_summaries = []\n",
        "            for j in range(len(args.scoring_methods)):\n",
        "                scored_summaries_path_j = \"../../scored_summaries/{}/{}/{}/{}/{}_scored_summaries_{}_{}_beams_{}.pkl\".format(\n",
        "                    args.dataset, set, generation_method, args.scoring_methods[j],\n",
        "                    set, args.model_name, size, args.n_beams\n",
        "                )\n",
        "                print(scored_summaries_path_j)\n",
        "                with open(scored_summaries_path_j, \"rb\") as f:\n",
        "                    scored_summaries = pickle.load(f)\n",
        "                gen_scored_summaries.append(scored_summaries)\n",
        "            scored_summaries = []\n",
        "            for i in range(len(gen_scored_summaries[0])):\n",
        "                summaries_i = gen_scored_summaries[0][i][0]\n",
        "                scores_i = []\n",
        "                for j in range(len(args.scoring_methods)):\n",
        "                    scores_i_j = gen_scored_summaries[j][i][1]\n",
        "                    scores_i.append(scores_i_j)\n",
        "                scored_summaries.append((summaries_i, scores_i))\n",
        "            print(len(scored_summaries), len(scored_summaries[0]), len(scored_summaries[0][0]), len(scored_summaries[0][1]), len(scored_summaries[0][1][0]))\n",
        "            all_scored_summaries.append(scored_summaries)\n",
        "        scored_summaries = combine_summaries(all_scored_summaries)\n",
        "\n",
        "    print(\"Total # of texts: {}, labels: {}, summary_candidates: {}, # candidates / text: {}\".format(\n",
        "        len(texts), len(summaries), len(scored_summaries), len(scored_summaries[0][0])))\n",
        "\n",
        "    return texts, summaries, scored_summaries\n",
        "def read_data_files_individual(set, train=False):\n",
        "    texts = []\n",
        "    summaries = []\n",
        "    if train:\n",
        "        for set_ in set:\n",
        "            set_text_path = \"../../data/{}/{}_text.txt\".format(dataset, set_)\n",
        "            set_summary_path = \"../../data/{}/{}_summary.txt\".format(dataset, set_)\n",
        "            n_docs = len(os.listdir(set_text_path))\n",
        "            print(\"There are {} {} documents\".format(n_docs, set_))\n",
        "            for i in tqdm(range(n_docs)):\n",
        "                text_path_i = set_text_path + \"{}_text_{}.txt\".format(set_, i)\n",
        "                text_i = \"\".join(open(text_path_i, \"r\").readlines())\n",
        "                texts.append(text_i)\n",
        "            for i in tqdm(range(n_docs)):\n",
        "                summary_path_i = set_summary_path + \"{}_summary_{}.txt\".format(set_, i)\n",
        "                summary_i = \"\".join(open(summary_path_i, \"r\").readlines())\n",
        "                summaries.append(summary_i)\n",
        "    else:\n",
        "        set_text_path = \"../../data/{}/{}_text.txt\".format(dataset, set)\n",
        "        set_summary_path = \"../../data/{}/{}_summary.txt\".format(dataset, set)\n",
        "        n_docs = len(os.listdir(set_text_path))\n",
        "        print(\"There are {} {} documents\".format(n_docs, set))\n",
        "        for i in tqdm(range(n_docs)):\n",
        "            text_path_i = set_text_path + \"{}_text_{}.txt\".format(set, i)\n",
        "            text_i = \"\".join(open(text_path_i, \"r\").readlines())\n",
        "            texts.append(text_i)\n",
        "        for i in tqdm(range(n_docs)):\n",
        "            summary_path_i = set_summary_path + \"{}_summary_{}.txt\".format(set, i)\n",
        "            summary_i = \"\".join(open(summary_path_i, \"r\").readlines())\n",
        "            summaries.append(summary_i)\n",
        "\n",
        "    return texts, summaries\n",
        "\n",
        "\n",
        "def prepare_data_files(set, dataset, train):\n",
        "    text_files = []\n",
        "    summary_files = []\n",
        "    if train:\n",
        "        for set_ in set:\n",
        "            text_file = \"../../data/{}/{}_text.txt\".format(dataset, set_)\n",
        "            summary_file = \"../../data/{}/{}_summary.txt\".format(dataset, set_)\n",
        "            text_files.append(text_file)\n",
        "            summary_files.append(summary_file)\n",
        "    else:\n",
        "        text_file = \"../../data/{}/{}_text.txt\".format(dataset, set)\n",
        "        summary_file = \"../../data/{}/{}_summary.txt\".format(dataset, set)\n",
        "        text_files.append(text_file)\n",
        "        summary_files.append(summary_file)\n",
        "\n",
        "    print(\"For set {}, loading the following files:\".format(set))\n",
        "    print(text_files)\n",
        "    print(summary_files)\n",
        "\n",
        "    return text_files, summary_files\n",
        "\n",
        "\n",
        "def read_data_files(text_files, summary_files):\n",
        "    # read the .txt files\n",
        "    texts = []\n",
        "    summaries = []\n",
        "\n",
        "    for text_file in text_files:\n",
        "        lines = read_one_file(text_file)\n",
        "        texts += lines\n",
        "    for summary_file in summary_files:\n",
        "        lines = read_one_file(summary_file)\n",
        "        summaries += lines\n",
        "\n",
        "    return texts, summaries\n",
        "\n",
        "\n",
        "def read_one_file(file):\n",
        "    lines = []\n",
        "    with open(file, 'r') as f:\n",
        "        for l in tqdm(f.readlines()):\n",
        "            lines.append(l)\n",
        "    print(file, len(lines))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def combine_summaries(all_scored_summaries):\n",
        "    res = []\n",
        "    for i in tqdm(range(len(all_scored_summaries[0]))):\n",
        "        summaries_i = []\n",
        "        scores_i = []\n",
        "        for k in range(len(all_scored_summaries[0][i][1])):\n",
        "            scores_i.append([])\n",
        "        for j in range(len(all_scored_summaries)):\n",
        "            summaries_i_j = all_scored_summaries[j][i][0]\n",
        "            summaries_i += summaries_i_j\n",
        "            scores_i_j = all_scored_summaries[j][i][1]\n",
        "            for k in range(len(scores_i_j)):\n",
        "                scores_i[k] += scores_i_j[k]\n",
        "        res.append((summaries_i, scores_i))\n",
        "\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "0qzwQFtPT8O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataloaders\n",
        "\n",
        "class MultitaskRerankingDataset:\n",
        "    def __init__(self, mode, tokenizer, texts, scored_summaries, labels, args):\n",
        "        self.mode = mode\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.scored_summaries = scored_summaries\n",
        "        self.labels = labels\n",
        "        self.args = args\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        label = self.labels[item]\n",
        "        scored_summaries = self.scored_summaries[item]\n",
        "        summary_candidates = scored_summaries[0]\n",
        "        summary_scores = scored_summaries[1]\n",
        "        for i in range(len(summary_scores)):\n",
        "            # Re-adjust for BERTScore\n",
        "            if min(summary_scores[i]) > 0.0 and max(summary_scores[i]) < 1.0:\n",
        "              for j in range(len(summary_scores[i])):\n",
        "                summary_scores[i][j] *= 100\n",
        "            # Re-adjust for BARTScore\n",
        "            if min(summary_scores[i]) > -10.0 and max(summary_scores[i]) < 0.0:\n",
        "              for j in range(len(summary_scores[i])):\n",
        "                summary_scores[i][j] *= 30\n",
        "\n",
        "        text_inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=self.args.max_length, padding='max_length')\n",
        "        text_inputs[\"input_ids\"] = text_inputs[\"input_ids\"][:, :self.args.max_length]\n",
        "        text_inputs[\"attention_mask\"] = text_inputs[\"attention_mask\"][:, :self.args.max_length]\n",
        "        summary_candidates_inputs = self.tokenizer(summary_candidates, return_tensors=\"pt\", truncation=True, max_length=self.args.max_summary_length, padding='max_length')\n",
        "        summary_candidates_inputs[\"input_ids\"] = summary_candidates_inputs[\"input_ids\"][:,:self.args.max_summary_length]\n",
        "        summary_candidates_inputs[\"attention_mask\"] = summary_candidates_inputs[\"attention_mask\"][:,:self.args.max_summary_length]\n",
        "\n",
        "        text_and_summaries = [self.tokenizer.decode(text_inputs[\"input_ids\"][0], skip_special_tokens=True) + \" \" + self.args.sep_symbol + \" \" \\\n",
        "                              + self.tokenizer.decode(summary_candidates_inputs[\"input_ids\"][i], skip_special_tokens=True) for i in range(len(summary_candidates_inputs[\"input_ids\"]))]\n",
        "        text_and_summaries_inputs = self.tokenizer(text_and_summaries, return_tensors=\"pt\", truncation=True, max_length=self.args.max_length + self.args.max_summary_length, padding='max_length')\n",
        "        text_and_summaries_inputs[\"input_ids\"] = text_and_summaries_inputs[\"input_ids\"][:, :(self.args.max_length + self.args.max_summary_length)]\n",
        "        text_and_summaries_inputs[\"attention_mask\"] = text_and_summaries_inputs[\"attention_mask\"][:, :(self.args.max_length + self.args.max_summary_length)]\n",
        "\n",
        "        scores = torch.cat([torch.tensor(summary_scores[i]).unsqueeze(0) for i in range(len(summary_scores))], 0)\n",
        "        labels = torch.max(scores, dim = 1)[0]\n",
        "        mode = torch.tensor([1])\n",
        "        if self.mode != \"train\":\n",
        "            mode = torch.tensor([0])\n",
        "\n",
        "        batch = {\n",
        "            \"mode\": mode,\n",
        "            \"text\": text,\n",
        "            \"label\": label,\n",
        "            \"text_and_summaries_input_ids\": text_and_summaries_inputs[\"input_ids\"],\n",
        "            \"text_and_summaries_attn_mask\": text_and_summaries_inputs[\"attention_mask\"],\n",
        "            \"scores\": scores,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "        return batch\n",
        "\n",
        "class MultitaskRerankingDatasetTrain:\n",
        "  def __init__(self, mode, tokenizer, texts, scored_summaries, labels, args):\n",
        "    self.mode = mode\n",
        "    self.tokenizer = tokenizer\n",
        "    self.texts = texts\n",
        "    self.scored_summaries = scored_summaries\n",
        "    self.labels = labels\n",
        "    self.args = args\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = self.texts[item]\n",
        "    scored_summaries = self.scored_summaries[item]\n",
        "    summary_candidates = scored_summaries[0]\n",
        "    summary_scores = scored_summaries[1]\n",
        "    for i in range(len(summary_scores)):\n",
        "      # re-adjust BERTScore\n",
        "      if min(summary_scores[i]) > 0.0 and max(summary_scores[i]) < 1.0:\n",
        "        for j in range(len(summary_scores[i])):\n",
        "          summary_scores[i][j] *= 100\n",
        "      # re-adjust BARTScore\n",
        "      elif min(summary_scores[i]) > -10.0 and max(summary_scores[i]) < 0.0:\n",
        "        for j in range(len(summary_scores[i])):\n",
        "          summary_scores[i][j] *= 30\n",
        "\n",
        "    text_inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=self.args.max_length, padding='max_length')\n",
        "    text_inputs[\"input_ids\"] = text_inputs[\"input_ids\"][:, :self.args.max_length]\n",
        "    text_inputs[\"attention_mask\"] = text_inputs[\"attention_mask\"][:, :self.args.max_length]\n",
        "\n",
        "    summary_candidates_inputs = self.tokenizer(summary_candidates, return_tensors=\"pt\", truncation=True, max_length=self.args.max_summary_length, padding='max_length')\n",
        "    summary_candidates_inputs[\"input_ids\"] = summary_candidates_inputs[\"input_ids\"][:,:self.args.max_summary_length]\n",
        "    summary_candidates_inputs[\"attention_mask\"] = summary_candidates_inputs[\"attention_mask\"][:,:self.args.max_summary_length]\n",
        "\n",
        "    text_and_summaries = [self.tokenizer.decode(text_inputs[\"input_ids\"][0], skip_special_tokens=True) + \" \" + self.args.sep_symbol + \" \" \\\n",
        "                          + self.tokenizer.decode(summary_candidates_inputs[\"input_ids\"][i], skip_special_tokens=True) for i in range(len(summary_candidates_inputs[\"input_ids\"]))]\n",
        "    text_and_summaries_inputs = self.tokenizer(text_and_summaries, return_tensors=\"pt\", truncation=True, max_length=self.args.max_length + self.args.max_summary_length, padding='max_length')\n",
        "    text_and_summaries_inputs[\"input_ids\"] = text_and_summaries_inputs[\"input_ids\"][:, :(self.args.max_length + self.args.max_summary_length)]\n",
        "    text_and_summaries_inputs[\"attention_mask\"] = text_and_summaries_inputs[\"attention_mask\"][:, :(self.args.max_length + self.args.max_summary_length)]\n",
        "\n",
        "    scores = torch.cat([torch.tensor(summary_scores[i]).unsqueeze(0) for i in range(len(summary_scores))], 0)\n",
        "    labels = torch.max(scores, dim = 1)[0]\n",
        "    mode = torch.tensor([1])\n",
        "    if self.mode != \"train\":\n",
        "        mode = torch.tensor([0])\n",
        "\n",
        "    batch = {\n",
        "        \"mode\": mode,\n",
        "        \"text_and_summaries_input_ids\": text_and_summaries_inputs[\"input_ids\"],\n",
        "        \"text_and_summaries_attn_mask\": text_and_summaries_inputs[\"attention_mask\"],\n",
        "        \"scores\": scores,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "    return batch\n",
        "\n",
        "def pre_rouge_processing(summary, args):\n",
        "    if args.clean_n:\n",
        "      summary = summary.replace(\"<n>\", \" \")\n",
        "    if args.highlights:\n",
        "      summary = \"\\n\".join(sent_tokenize(summary))\n",
        "    return summary"
      ],
      "metadata": {
        "id": "_tHDmky3HiSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0WolSHtLJ4m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL\n",
        "class MoE(nn.Module):\n",
        "  \"\"\"Call a Sparsely gated mixture of experts layer with 1-layer Feed-Forward networks as experts.\n",
        "  Args:\n",
        "  input_size: integer - size of the input\n",
        "  output_size: integer - size of the input\n",
        "  num_experts: an integer - number of experts\n",
        "  hidden_size: an integer - hidden size of the experts\n",
        "  noisy_gating: a boolean\n",
        "  k: an integer - how many experts to use for each batch element\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, device, n_tasks, input_size, output_size, num_experts, hidden_size, k=4):\n",
        "    super(MoE, self).__init__()\n",
        "    self.device = device\n",
        "    self.n_tasks = n_tasks\n",
        "    self.num_experts = num_experts\n",
        "    self.output_size = output_size\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.k = k\n",
        "    # instantiate experts\n",
        "    self.experts = nn.ModuleList([MLPExpert(self.input_size, self.output_size, self.hidden_size) for i in range(self.num_experts)])\n",
        "    self.w_gate = nn.ParameterList([nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True) for i in range(n_tasks)])\n",
        "    self.w_noise = nn.ParameterList([nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True) for i in range(n_tasks)])\n",
        "\n",
        "    self.softplus = nn.Softplus()\n",
        "    self.softmax = nn.Softmax(1)\n",
        "    self.normal = Normal(torch.tensor([0.0]).to(device), torch.tensor([1.0]).to(device))\n",
        "\n",
        "    assert(self.k <= self.num_experts)\n",
        "\n",
        "    self.init_tasks_probs()\n",
        "\n",
        "  def init_tasks_probs(self):\n",
        "    self.tasks_probs = []\n",
        "    for j in range(self.n_tasks):\n",
        "      temp = []\n",
        "      for i in range(self.num_experts):\n",
        "          temp.append([])\n",
        "      self.tasks_probs.append(temp)\n",
        "\n",
        "  def display_tasks_probs(self):\n",
        "    print(\"\\nProbability distribution on experts for each task, computed over {} data points:\".format(len(self.tasks_probs[0][0])))\n",
        "    for j in range(self.n_tasks):\n",
        "      probs = self.tasks_probs[j]\n",
        "      probs = np.array([np.mean(x) for x in probs])\n",
        "      prob_std = np.std(probs)\n",
        "      probs = [\"{:.4f}\".format(x) for x in probs]\n",
        "      print(\"Task {} / {}, distribution across experts: {}, std: {:.4f}\".format(j+1, self.n_tasks, probs, prob_std))\n",
        "    self.init_tasks_probs()\n",
        "\n",
        "  def cv_squared(self, x):\n",
        "    \"\"\"The squared coefficient of variation of a sample.\n",
        "    Useful as a loss to encourage a positive distribution to be more uniform.\n",
        "    Epsilons added for numerical stability.\n",
        "    Returns 0 for an empty Tensor.\n",
        "    Args:\n",
        "    x: a `Tensor`.\n",
        "    Returns:\n",
        "    a `Scalar`.\n",
        "    \"\"\"\n",
        "    eps = 1e-10\n",
        "    # if only num_experts = 1\n",
        "    if x.shape[0] == 1:\n",
        "        return torch.Tensor([0])\n",
        "    return x.float().var() / (x.float().mean()**2 + eps)\n",
        "\n",
        "  def _gates_to_load(self, gates):\n",
        "    \"\"\"Compute the true load per expert, given the gates.\n",
        "    The load is the number of examples for which the corresponding gate is >0.\n",
        "    Args:\n",
        "    gates: a `Tensor` of shape [batch_size, n]\n",
        "    Returns:\n",
        "    a float32 `Tensor` of shape [n]\n",
        "    \"\"\"\n",
        "    return (gates > 0).sum(0)\n",
        "\n",
        "  def _prob_in_top_k(self, clean_values, noisy_values, noise_stddev, noisy_top_values):\n",
        "    \"\"\"Helper function to NoisyTopKGating.\n",
        "    Computes the probability that value is in top k, given different random noise.\n",
        "    This gives us a way of backpropagating from a loss that balances the number\n",
        "    of times each expert is in the top k experts per example.\n",
        "    In the case of no noise, pass in None for noise_stddev, and the result will\n",
        "    not be differentiable.\n",
        "    Args:\n",
        "    clean_values: a `Tensor` of shape [batch, n].\n",
        "    noisy_values: a `Tensor` of shape [batch, n].  Equal to clean values plus\n",
        "      normally distributed noise with standard deviation noise_stddev.\n",
        "    noise_stddev: a `Tensor` of shape [batch, n], or None\n",
        "    noisy_top_values: a `Tensor` of shape [batch, m].\n",
        "        \"values\" Output of tf.top_k(noisy_top_values, m).  m >= k+1\n",
        "    Returns:\n",
        "    a `Tensor` of shape [batch, n].\n",
        "    \"\"\"\n",
        "\n",
        "    batch = clean_values.size(0)\n",
        "    m = noisy_top_values.size(1)\n",
        "    top_values_flat = noisy_top_values.flatten()\n",
        "    threshold_positions_if_in = torch.arange(batch) * m + self.k\n",
        "    threshold_positions_if_in = threshold_positions_if_in.to(self.device)\n",
        "    threshold_if_in = torch.unsqueeze(torch.gather(top_values_flat, 0, threshold_positions_if_in), 1)\n",
        "    is_in = torch.gt(noisy_values, threshold_if_in)\n",
        "    threshold_positions_if_out = threshold_positions_if_in - 1\n",
        "    threshold_if_out = torch.unsqueeze(torch.gather(top_values_flat,0 , threshold_positions_if_out), 1)\n",
        "    # is each value currently in the top k.\n",
        "    prob_if_in = self.normal.cdf((clean_values - threshold_if_in)/noise_stddev)\n",
        "    prob_if_out = self.normal.cdf((clean_values - threshold_if_out)/noise_stddev)\n",
        "    prob = torch.where(is_in, prob_if_in, prob_if_out)\n",
        "    return prob\n",
        "\n",
        "  def noisy_top_k_gating(self, gate_idx, x, train, noise_epsilon=1e-2):\n",
        "    \"\"\"Noisy top-k gating.\n",
        "      See paper: https://arxiv.org/abs/1701.06538.\n",
        "      Args:\n",
        "        x: input Tensor with shape [batch_size, input_size]\n",
        "        train: a boolean - we only add noise at training time.\n",
        "        noise_epsilon: a float\n",
        "      Returns:\n",
        "        gates: a Tensor with shape [batch_size, num_experts]\n",
        "        load: a Tensor with shape [num_experts]\n",
        "    \"\"\"\n",
        "    clean_logits = x @ self.w_gate[gate_idx]\n",
        "    if train:\n",
        "        raw_noise_stddev = x @ self.w_noise[gate_idx]\n",
        "        noise_stddev = ((self.softplus(raw_noise_stddev) + noise_epsilon))\n",
        "        noisy_logits = clean_logits + ( torch.randn_like(clean_logits) * noise_stddev)\n",
        "        logits = noisy_logits\n",
        "    else:\n",
        "        logits = clean_logits\n",
        "\n",
        "    # calculate topk + 1 that will be needed for the noisy gates\n",
        "    top_logits, top_indices = logits.topk(min(self.k + 1, self.num_experts), dim=1)\n",
        "    top_k_logits = top_logits[:, :self.k]\n",
        "    top_k_indices = top_indices[:, :self.k]\n",
        "    top_k_gates = self.softmax(top_k_logits)\n",
        "\n",
        "    zeros = torch.zeros_like(logits, requires_grad=True)\n",
        "    gates = zeros.scatter(1, top_k_indices, top_k_gates)\n",
        "\n",
        "    if train and self.k < self.num_experts:\n",
        "        load = (self._prob_in_top_k(clean_logits, noisy_logits, noise_stddev, top_logits)).sum(0)\n",
        "    else:\n",
        "        load = self._gates_to_load(gates)\n",
        "    return gates, load\n",
        "\n",
        "  def forward(self, x, train=True, collect_gates = False, loss_coef=1e-2):\n",
        "      \"\"\"Args:\n",
        "      x: tensor shape [batch_size, input_size]\n",
        "      train: a boolean scalar.\n",
        "      loss_coef: a scalar - multiplier on load-balancing losses\n",
        "      Returns:\n",
        "      y: a tensor with shape [batch_size, output_size].\n",
        "      extra_training_loss: a scalar.  This should be added into the overall\n",
        "      training loss of the model.  The backpropagation of this loss\n",
        "      encourages all experts to be approximately equally used across a batch.\n",
        "      \"\"\"\n",
        "      all_y = []\n",
        "      all_loss = torch.tensor(0.0).cuda()\n",
        "      for gate_idx in range(self.n_tasks):\n",
        "          gates, load = self.noisy_top_k_gating(gate_idx, x, train)\n",
        "          # calculate importance loss\n",
        "          importance = gates.sum(0)\n",
        "\n",
        "          if collect_gates == True:\n",
        "              t = gates.detach().cpu().numpy()\n",
        "              for i in range(t.shape[1]):\n",
        "                  self.tasks_probs[gate_idx][i] += list(t[:,i])\n",
        "\n",
        "          loss = self.cv_squared(importance) + self.cv_squared(load)\n",
        "          loss *= loss_coef\n",
        "\n",
        "          dispatcher = SparseDispatcher(self.device, self.num_experts, gates)\n",
        "          expert_inputs = dispatcher.dispatch(x)\n",
        "          gates = dispatcher.expert_to_gates()\n",
        "          expert_outputs = [self.experts[i](expert_inputs[i]) for i in range(self.num_experts)]\n",
        "          y = dispatcher.combine(expert_outputs)\n",
        "\n",
        "          all_y.append(y)\n",
        "          all_loss = all_loss + loss\n",
        "\n",
        "      return all_y, all_loss\n",
        "class MLPExpert(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(MLPExpert, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MLPTower(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MLPTower, self).__init__()\n",
        "        #self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "        #self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc2(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SparseDispatcher(object):\n",
        "    \"\"\"Helper for implementing a mixture of experts.\n",
        "    The purpose of this class is to create input minibatches for the\n",
        "    experts and to combine the results of the experts to form a unified\n",
        "    output tensor.\n",
        "    There are two functions:\n",
        "    dispatch - take an input Tensor and create input Tensors for each expert.\n",
        "    combine - take output Tensors from each expert and form a combined output\n",
        "      Tensor.  Outputs from different experts for the same batch element are\n",
        "      summed together, weighted by the provided \"gates\".\n",
        "    The class is initialized with a \"gates\" Tensor, which specifies which\n",
        "    batch elements go to which experts, and the weights to use when combining\n",
        "    the outputs.  Batch element b is sent to expert e iff gates[b, e] != 0.\n",
        "    The inputs and outputs are all two-dimensional [batch, depth].\n",
        "    Caller is responsible for collapsing additional dimensions prior to\n",
        "    calling this class and reshaping the output to the original shape.\n",
        "    See common_layers.reshape_like().\n",
        "    Example use:\n",
        "    gates: a float32 `Tensor` with shape `[batch_size, num_experts]`\n",
        "    inputs: a float32 `Tensor` with shape `[batch_size, input_size]`\n",
        "    experts: a list of length `num_experts` containing sub-networks.\n",
        "    dispatcher = SparseDispatcher(num_experts, gates)\n",
        "    expert_inputs = dispatcher.dispatch(inputs)\n",
        "    expert_outputs = [experts[i](expert_inputs[i]) for i in range(num_experts)]\n",
        "    outputs = dispatcher.combine(expert_outputs)\n",
        "    The preceding code sets the output for a particular example b to:\n",
        "    output[b] = Sum_i(gates[b, i] * experts[i](inputs[b]))\n",
        "    This class takes advantage of sparsity in the gate matrix by including in the\n",
        "    `Tensor`s for expert i only the batch elements for which `gates[b, i] > 0`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, num_experts, gates):\n",
        "        \"\"\"Create a SparseDispatcher.\"\"\"\n",
        "\n",
        "        self.device = device\n",
        "        self._gates = gates\n",
        "        self._num_experts = num_experts\n",
        "        # sort experts\n",
        "        sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)\n",
        "        # drop indices\n",
        "        _, self._expert_index = sorted_experts.split(1, dim=1)\n",
        "        # get according batch index for each expert\n",
        "        self._batch_index = sorted_experts[index_sorted_experts[:, 1],0]\n",
        "        # calculate num samples that each expert gets\n",
        "        self._part_sizes = list((gates > 0).sum(0).detach().cpu().numpy())\n",
        "        # expand gates to match with self._batch_index\n",
        "        gates_exp = gates[self._batch_index.flatten()]\n",
        "        self._nonzero_gates = torch.gather(gates_exp, 1, self._expert_index)\n",
        "\n",
        "    def dispatch(self, inp):\n",
        "        \"\"\"Create one input Tensor for each expert.\n",
        "        The `Tensor` for a expert `i` contains the slices of `inp` corresponding\n",
        "        to the batch elements `b` where `gates[b, i] > 0`.\n",
        "        Args:\n",
        "          inp: a `Tensor` of shape \"[batch_size, <extra_input_dims>]`\n",
        "        Returns:\n",
        "          a list of `num_experts` `Tensor`s with shapes\n",
        "            `[expert_batch_size_i, <extra_input_dims>]`.\n",
        "        \"\"\"\n",
        "\n",
        "        # assigns samples to experts whose gate is nonzero\n",
        "\n",
        "        # expand according to batch index so we can just split by _part_sizes\n",
        "        inp_exp = inp[self._batch_index].squeeze(1)\n",
        "        return torch.split(inp_exp, self._part_sizes, dim=0)\n",
        "\n",
        "    def combine(self, expert_out, multiply_by_gates=True):\n",
        "        \"\"\"Sum together the expert output, weighted by the gates.\n",
        "        The slice corresponding to a particular batch element `b` is computed\n",
        "        as the sum over all experts `i` of the expert output, weighted by the\n",
        "        corresponding gate values.  If `multiply_by_gates` is set to False, the\n",
        "        gate values are ignored.\n",
        "        Args:\n",
        "          expert_out: a list of `num_experts` `Tensor`s, each with shape\n",
        "            `[expert_batch_size_i, <extra_output_dims>]`.\n",
        "          multiply_by_gates: a boolean\n",
        "        Returns:\n",
        "          a `Tensor` with shape `[batch_size, <extra_output_dims>]`.\n",
        "        \"\"\"\n",
        "        # apply exp to expert outputs, so we are not longer in log space\n",
        "        stitched = torch.cat(expert_out, 0).exp()\n",
        "\n",
        "        if multiply_by_gates:\n",
        "            stitched = stitched.mul(self._nonzero_gates)\n",
        "        zeros = torch.zeros(self._gates.size(0), expert_out[-1].size(1), requires_grad=True).to(self.device)\n",
        "        # combine samples that have been processed by the same k experts\n",
        "        combined = zeros.index_add(0, self._batch_index, stitched.float())\n",
        "        # add eps to all zero values in order to avoid nans when going back to log space\n",
        "        combined[combined == 0] = np.finfo(float).eps\n",
        "        # back to log space\n",
        "        return combined.log()\n",
        "\n",
        "    def expert_to_gates(self):\n",
        "        \"\"\"Gate values corresponding to the examples in the per-expert `Tensor`s.\n",
        "        Returns:\n",
        "          a list of `num_experts` one-dimensional `Tensor`s with type `tf.float32`\n",
        "              and shapes `[expert_batch_size_i]`\n",
        "        \"\"\"\n",
        "        # split nonzero gates for each expert\n",
        "        return torch.split(self._nonzero_gates, self._part_sizes, dim=0)\n",
        "\n",
        "class ModelMultitaskBinary(nn.Module):\n",
        "    def __init__(self, pretrained_model, tokenizer, args):\n",
        "        super(ModelMultitaskBinary, self).__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.args = args\n",
        "\n",
        "        # LM\n",
        "        self.pretrained_model = pretrained_model\n",
        "        # shared bottom\n",
        "        self.fc1 = nn.Linear(args.hidden_size, args.bottom_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(args.bottom_hidden_size, args.hidden_size)\n",
        "        # MoE\n",
        "        self.moe = MoE(args.device, args.n_tasks, args.hidden_size, args.hidden_size, args.num_experts, args.expert_hidden_size, args.k)\n",
        "        # towers - one for each task\n",
        "        self.towers = nn.ModuleList([MLPTower(args.hidden_size, args.tower_hidden_size) for i in range(args.n_tasks)])\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # sampled candidates\n",
        "        self.selected_idx = []\n",
        "\n",
        "        # training labels\n",
        "        self.original_training_labels = {}\n",
        "        self.training_labels = {}\n",
        "        self.training_scores = {}\n",
        "        self.training_hits = {}\n",
        "        for j in range(self.args.n_tasks):\n",
        "            self.original_training_labels[j] = []\n",
        "            self.training_labels[j] = []\n",
        "            self.training_scores[j] = []\n",
        "            self.training_hits[j] = []\n",
        "\n",
        "        # multi-summary evaluation\n",
        "        self.multi_summary_pred_idx = {}\n",
        "        self.multi_summary_preds = {}\n",
        "        for j in range(self.args.n_tasks):\n",
        "            self.multi_summary_pred_idx[j] = []\n",
        "            self.multi_summary_preds[j] = []\n",
        "\n",
        "    def forward(self, mode, text_and_summaries_ids, text_and_summaries_mask, scores):\n",
        "        loss = torch.tensor(0.0).to(self.pretrained_model.device)\n",
        "        accuracy = [0 for j in range(self.args.n_tasks)]\n",
        "        rank = [0 for j in range(self.args.n_tasks)]\n",
        "        predictions_idx = [[] for j in range(self.args.n_tasks)]\n",
        "        predictions = [[] for j in range(self.args.n_tasks)]\n",
        "        total_predictions_idx = []\n",
        "        overall_sums = []\n",
        "        overall_predictions = []\n",
        "        for i in range(text_and_summaries_ids.shape[0]):\n",
        "\n",
        "            # data\n",
        "            text_and_summaries_ids_i = text_and_summaries_ids[i]\n",
        "            text_and_summaries_mask_i = text_and_summaries_mask[i]\n",
        "\n",
        "            # labels construction\n",
        "            scores_i = scores[i]\n",
        "            original_scores_i = scores_i.clone().detach()\n",
        "            labels_i = torch.zeros(scores_i.shape, device = self.pretrained_model.device)\n",
        "            for j in range(self.args.n_tasks):\n",
        "                best_j = scores_i[j].max()\n",
        "                if self.args.sharp_pos:\n",
        "                    if best_j > scores_i[j].min():\n",
        "                        labels_i[j][scores_i[j] == best_j] = 1\n",
        "                else:\n",
        "                    labels_i[j][scores_i[j] == best_j] = 1\n",
        "            original_labels_i = labels_i.clone().detach()\n",
        "\n",
        "            # candidate sampling\n",
        "            selected_idx, text_and_summaries_ids_i, text_and_summaries_mask_i, scores_i, labels_i = candidate_subsampling(\n",
        "                mode, text_and_summaries_ids_i, text_and_summaries_mask_i, scores_i, labels_i, self.args\n",
        "            )\n",
        "            self.selected_idx += selected_idx\n",
        "\n",
        "            # model output\n",
        "            # LM encoding\n",
        "            outputs_i = self.pretrained_model(\n",
        "                input_ids = text_and_summaries_ids_i, attention_mask = text_and_summaries_mask_i, output_hidden_states = True\n",
        "            )\n",
        "            encs = outputs_i[\"last_hidden_state\"]\n",
        "            encs = encs[:, 0, :]\n",
        "            # shared bottom\n",
        "            if self.args.use_shared_bottom:\n",
        "                preds_i = self.fc2(self.relu(self.fc1(encs)))\n",
        "            else:\n",
        "                preds_i = encs\n",
        "            # MoE\n",
        "            train = torch.sum(mode) > 0\n",
        "            preds_i, aux_loss_i = self.moe(preds_i, train = train, collect_gates = not(train))\n",
        "\n",
        "            loss_i = torch.tensor(0.0).to(self.pretrained_model.device)\n",
        "            total_predictions = np.zeros(len(preds_i[0]))\n",
        "            for j in range(self.args.n_tasks):\n",
        "\n",
        "                # pred\n",
        "                preds_i_j = self.towers[j](preds_i[j])[:, 0]\n",
        "\n",
        "                # labels\n",
        "                labels_i_j = labels_i[j]\n",
        "                if torch.sum(mode) > 0:\n",
        "                    self.original_training_labels[j].append(original_labels_i[j].sum().item())\n",
        "                    self.training_labels[j].append(labels_i_j.sum().item())\n",
        "                    if labels_i_j.sum() > 0:\n",
        "                        self.training_scores[j].append(scores_i[j][labels_i_j == 1].mean().item())\n",
        "                    self.training_hits[j].append(int(scores_i[j].max().item() == original_scores_i[j].max().item()))\n",
        "\n",
        "                # loss\n",
        "                loss_i_j = self.loss(preds_i_j, labels_i_j)\n",
        "                loss_i = loss_i + loss_i_j\n",
        "\n",
        "                # predictions\n",
        "                preds_i_j = self.sigmoid(preds_i_j).detach().cpu().numpy()\n",
        "                prediction_idx = np.argmax(preds_i_j)\n",
        "                predictions_idx[j].append(prediction_idx)\n",
        "                prediction = scores_i[j][prediction_idx].item()\n",
        "                predictions[j].append(prediction)\n",
        "                total_predictions += preds_i_j\n",
        "\n",
        "                # accuracy\n",
        "                pos_idx = scores_i[j].argmax().item()\n",
        "                accuracy_i_j = 100 * int(scores_i[j][prediction_idx].item() == scores_i[j][pos_idx].item())\n",
        "                accuracy[j] = accuracy[j] + accuracy_i_j\n",
        "\n",
        "                # ranks\n",
        "                ranks = rank_array(preds_i_j)\n",
        "                all_pos_idx = [k for k in range(len(scores_i[j])) if scores_i[j][k].item() == scores_i[j][pos_idx].item()]\n",
        "                rank_i_j = np.min(ranks[all_pos_idx])\n",
        "                rank[j] = rank[j] + rank_i_j\n",
        "            loss_i = loss_i / self.args.n_tasks\n",
        "            if self.args.use_aux_loss:\n",
        "                loss_i = loss_i + aux_loss_i\n",
        "            loss = loss + loss_i\n",
        "            total_predictions /= self.args.n_tasks\n",
        "            total_prediction_idx = np.argmax(total_predictions)\n",
        "            total_predictions_idx.append(total_prediction_idx)\n",
        "            overall_sum = sum([scores_i[j][total_prediction_idx].item() for j in range(self.args.n_tasks)])\n",
        "            overall_sums.append(overall_sum)\n",
        "            overall_predictions.append(total_predictions)\n",
        "\n",
        "        loss /= scores.shape[0]\n",
        "        outputs = {\n",
        "            \"loss\": loss,\n",
        "            \"loss_nce\": loss,\n",
        "            \"total_predictions_idx\": total_predictions_idx,\n",
        "            \"overall_predictions\": overall_predictions\n",
        "        }\n",
        "        prediction_sum = 0\n",
        "        for j in range(self.args.n_tasks):\n",
        "            accuracy[j] /= scores.shape[0]\n",
        "            outputs[\"accuracy_{}\".format(self.args.scoring_methods[j])] = torch.tensor(accuracy[j]).float().to(loss.device)\n",
        "            rank[j] /= scores.shape[0]\n",
        "            outputs[\"rank_{}\".format(self.args.scoring_methods[j])] = torch.tensor(rank[j]).float().to(loss.device)\n",
        "            if torch.sum(mode) == 0:\n",
        "                self.multi_summary_pred_idx[j] += predictions_idx[j]\n",
        "                self.multi_summary_preds[j] += predictions[j]\n",
        "            predictions[j] = np.mean(predictions[j])\n",
        "            outputs[\"prediction_{}\".format(self.args.scoring_methods[j])] = torch.tensor(predictions[j]).float().to(loss.device)\n",
        "            prediction_sum += predictions[j]\n",
        "        outputs[\"prediction_sum\"] = torch.tensor(prediction_sum).float().to(loss.device)\n",
        "        outputs[\"overall_sum\"] = torch.tensor(np.mean(overall_sums)).float().to(loss.device)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "g9z7njUDJ4fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Custom model trainer\n",
        "# #made for creating a custom output for evaluation\n",
        "# class Trainer:\n",
        "#     def __init__(self, *args, **kwargs):\n",
        "#         requires_backends(self, [\"torch\"])\n",
        "\n",
        "# class CustomTrainer(Trainer):\n",
        "#   def compute_loss(self, model, inputs, return_outputs=False):\n",
        "#     mode = inputs[\"mode\"]\n",
        "#     text_and_summaries_ids = inputs[\"text_and_summaries_input_ids\"]\n",
        "#     text_and_summaries_mask = inputs[\"text_and_summaries_attn_mask\"]\n",
        "#     scores = inputs[\"scores\"]\n",
        "\n",
        "#     outputs = model(mode, text_and_summaries_ids, text_and_summaries_mask, scores)\n",
        "\n",
        "#     loss = outputs[\"loss\"]\n",
        "#     output = torch.zeros(2 + 3 * args.n_tasks + 2).float().to(loss.device)\n",
        "#     output[0] = loss\n",
        "#     output[1] = outputs[\"loss_nce\"]\n",
        "#     for j in range(args.n_tasks):\n",
        "#         output[2 + j * 3] = outputs[\"accuracy_{}\".format(args.scoring_methods[j])]\n",
        "#         output[3 + j * 3] = outputs[\"rank_{}\".format(args.scoring_methods[j])]\n",
        "#         output[4 + j * 3] = outputs[\"prediction_{}\".format(args.scoring_methods[j])]\n",
        "#     output[-2] = outputs[\"prediction_sum\"]\n",
        "#     output[-1] = outputs[\"overall_sum\"]\n",
        "\n",
        "#     return (loss, output) if return_outputs else loss\n",
        "\n",
        "#   def prediction_step( self,\n",
        "#           model: nn.Module,\n",
        "#           inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "#           prediction_loss_only: bool,\n",
        "#           ignore_keys: Optional[List[str]] = None,\n",
        "#   ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "#     \"\"\"\n",
        "#     Perform an evaluation step on :obj:`model` using obj:`inputs`.\n",
        "\n",
        "#     Subclass and override to inject custom behavior.\n",
        "\n",
        "#     Args:\n",
        "#         model (:obj:`nn.Module`):\n",
        "#             The model to evaluate.\n",
        "#         inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
        "#             The inputs and targets of the model.\n",
        "\n",
        "#             The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
        "#             argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
        "#         prediction_loss_only (:obj:`bool`):\n",
        "#             Whether or not to return the loss only.\n",
        "#         ignore_keys (:obj:`Lst[str]`, `optional`):\n",
        "#             A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
        "#             gathering predictions.\n",
        "\n",
        "#     Return:\n",
        "#         Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
        "#         logits and labels (each being optional).\n",
        "#     \"\"\"\n",
        "#     has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
        "#     inputs = self._prepare_inputs(inputs)\n",
        "#     if ignore_keys is None:\n",
        "#         if hasattr(self.model, \"config\"):\n",
        "#             ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
        "#         else:\n",
        "#             ignore_keys = []\n",
        "\n",
        "#     # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
        "#     if has_labels:\n",
        "#         labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
        "#         if len(labels) == 1:\n",
        "#             labels = labels[0]\n",
        "#     else:\n",
        "#         labels = None\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         if has_labels:\n",
        "#             loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
        "#             loss = loss.mean().detach()\n",
        "#             if isinstance(outputs, dict):\n",
        "#                 logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
        "#             else:\n",
        "#                 logits = outputs[1:]\n",
        "#         else:\n",
        "#             loss = None\n",
        "#             if self.use_amp:\n",
        "#                 # with autocast():\n",
        "#                 outputs = model(**inputs)\n",
        "#             else:\n",
        "#                 text_inputs_ids = inputs[\"text_inputs_ids\"]\n",
        "#                 text_attention_mask = inputs[\"text_attention_mask\"]\n",
        "#                 text_inputs = {\n",
        "#                     \"input_ids\": text_inputs_ids,\n",
        "#                     \"attention_mask\": text_attention_mask\n",
        "#                 }\n",
        "#                 outputs = model(**text_inputs)\n",
        "#             if isinstance(outputs, dict):\n",
        "#                 logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
        "#             else:\n",
        "#                 logits = outputs\n",
        "#             # TODO: this needs to be fixed and made cleaner later.\n",
        "#             if self.args.past_index >= 0:\n",
        "#                 self._past = outputs[self.args.past_index - 1]\n",
        "\n",
        "#     if prediction_loss_only:\n",
        "#         return (loss, None, None)\n",
        "\n",
        "#     logits = nested_detach(logits)\n",
        "#     if len(logits) == 1:\n",
        "#         logits = logits[0]\n",
        "\n",
        "#     return (loss, logits, labels)\n",
        "\n",
        "#   def get_train_dataloader(self) -> DataLoader:\n",
        "#     \"\"\"\n",
        "#     Returns the training :class:`~torch.utils.data.DataLoader`.\n",
        "\n",
        "#     Will use no sampler if :obj:`self.train_dataset` does not implement :obj:`__len__`, a random sampler (adapted\n",
        "#     to distributed training if necessary) otherwise.\n",
        "\n",
        "#     Subclass and override this method if you want to inject some custom behavior.\n",
        "#     \"\"\"\n",
        "#     if self.train_dataset is None:\n",
        "#       raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
        "\n",
        "#     train_dataset = self.train_dataset\n",
        "#     if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
        "#       train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
        "\n",
        "#     return DataLoader(\n",
        "#       train_dataset,\n",
        "#       batch_size=self.args.train_batch_size,\n",
        "#       shuffle=train_dataset.args.shuffle_train,\n",
        "#       collate_fn=self.data_collator,\n",
        "#       drop_last=self.args.dataloader_drop_last,\n",
        "#       num_workers=self.args.dataloader_num_workers,\n",
        "#       pin_memory=self.args.dataloader_pin_memory,\n",
        "#     )\n",
        "\n",
        "# def compute_metrics(eval_preds):\n",
        "#   preds, labels = eval_preds\n",
        "#   loss_nce = np.mean([preds[i] for i in range(0, len(preds), 1 + 3 * args.n_tasks + 2)])\n",
        "#   result = {\n",
        "#     \"loss_nce\": loss_nce\n",
        "#   }\n",
        "#   for j in range(args.n_tasks):\n",
        "#     accuracy_arr = [preds[i] for i in range(1 + j * 3, len(preds), 1 + 3 * args.n_tasks + 2)]\n",
        "#     accuracy = np.mean(accuracy_arr)\n",
        "#     rank_arr = [preds[i] for i in range(2 + j * 3, len(preds), 1 + 3 * args.n_tasks + 2)]\n",
        "#     rank = np.mean(rank_arr)\n",
        "#     prediction_arr = [preds[i] for i in range(3 + j * 3, len(preds), 1 + 3 * args.n_tasks + 2)]\n",
        "#     prediction = np.mean(prediction_arr)\n",
        "#     print(\"Task {}, # pred batches: {}\".format(j + 1, len(accuracy_arr)))\n",
        "#     result[\"accuracy_{}\".format(args.scoring_methods[j])] = accuracy\n",
        "#     result[\"rank_{}\".format(args.scoring_methods[j])] = rank\n",
        "#     result[\"prediction_{}\".format(args.scoring_methods[j])] = prediction\n",
        "#   prediction_sum = np.mean([preds[i] for i in range(1 + 3 * args.n_tasks, len(preds), 1 + 3 * args.n_tasks + 2)])\n",
        "#   result[\"prediction_sum\"] = prediction_sum\n",
        "#   overall_sum = np.mean([preds[i] for i in range(1 + 3 * args.n_tasks + 1, len(preds), 1 + 3 * args.n_tasks + 2)])\n",
        "#   result[\"overall_sum\"] = overall_sum\n",
        "\n",
        "#   return result"
      ],
      "metadata": {
        "id": "wnWGSk1j4MNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EVALS\n",
        "\n",
        "def overall_eval(val_texts, val_summaries, val_labels, args):\n",
        "    # ROUGE\n",
        "    all_score_names = []\n",
        "    all_scores = []\n",
        "    if args.eval_rouge:\n",
        "        r1, r2, rl = rouge_eval(\"true labels\", val_texts, val_summaries, val_labels, args)\n",
        "        all_scores.append(r1)\n",
        "        all_scores.append(r2)\n",
        "        all_scores.append(rl)\n",
        "        all_score_names += [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]\n",
        "    # BERTScore\n",
        "    if args.eval_bertscore:\n",
        "        bs = bertscore_eval(val_summaries, val_labels, args)\n",
        "        all_scores.append(bs)\n",
        "        all_score_names.append(\"BERTScore\")\n",
        "    # BARTScore\n",
        "    if args.eval_bartscore:\n",
        "        bas = bartscore_eval(val_summaries, val_labels, args)\n",
        "        all_scores.append(bas)\n",
        "        all_score_names.append(\"BARTScore\")\n",
        "    # Abstractiveness\n",
        "    if args.eval_new_ngram:\n",
        "        new_ngram_eval(val_texts, val_summaries, args)\n",
        "\n",
        "    return all_scores, all_score_names\n",
        "\n",
        "def rouge_eval(mode, val_texts, val_summaries, val_labels, args):\n",
        "    print(\"\\n\", \"*\"*10, \"1 - ROUGE evaluation with {}\".format(mode), \"*\"*10)\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer = args.stemmer)\n",
        "    all_r1s = []\n",
        "    all_r2s = []\n",
        "    all_rls = []\n",
        "    for i in range(len(val_summaries)):\n",
        "        summary = val_summaries[i]\n",
        "        summary = pre_rouge_processing(summary, args)\n",
        "        label = val_labels[i]\n",
        "        r1, r2, rl = get_rouge_scores(summary, label, scorer, args)\n",
        "        all_r1s.append(r1)\n",
        "        all_r2s.append(r2)\n",
        "        all_rls.append(rl)\n",
        "    all_r1s = 100 * np.array(all_r1s)\n",
        "    all_r2s = 100 * np.array(all_r2s)\n",
        "    all_rls = 100 * np.array(all_rls)\n",
        "    mean_r1 = np.mean(all_r1s)\n",
        "    mean_r2 = np.mean(all_r2s)\n",
        "    mean_rl = np.mean(all_rls)\n",
        "    mean_r = (mean_r1 + mean_r2 + mean_rl) / 3\n",
        "    print(\"Mean R: {:.4f}, R-1: {:.4f} (var: {:.4f}), R-2: {:.4f} (var: {:.4f}), R-L: {:.4f} (var: {:.4f})\".format(\n",
        "        mean_r, mean_r1, np.std(all_r1s), mean_r2, np.std(all_r2s), mean_rl, np.std(all_rls)))\n",
        "\n",
        "    return all_r1s, all_r2s, all_rls\n",
        "\n",
        "def get_rouge_scores(summary, label, scorer, args):\n",
        "  rouge_scores = scorer.score(label, summary)\n",
        "  r1 = rouge_scores[\"rouge1\"].fmeasure\n",
        "  r2 = rouge_scores[\"rouge2\"].fmeasure\n",
        "  rl = rouge_scores[\"rougeLsum\"].fmeasure\n",
        "\n",
        "  return r1, r2, rl\n"
      ],
      "metadata": {
        "id": "xASBv1D_C6kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#UTILS\n",
        "\n",
        "def rank_array(t):\n",
        "  y = np.copy(t)\n",
        "  y.sort()\n",
        "  y = y[::-1]\n",
        "  ranks = np.zeros(len(t))\n",
        "  flagged = np.zeros(len(t))\n",
        "  for i in range(len(t)):\n",
        "    el = t[i]\n",
        "    for j in range(len(t)):\n",
        "      if el == y[j] and flagged[j] == 0:\n",
        "        ranks[i] = j\n",
        "        flagged[j] = 1\n",
        "        break\n",
        "  return ranks\n",
        "\n",
        "def nested_detach(tensors):\n",
        "  if isinstance(tensors, (list, tuple)):\n",
        "    return type(tensors)(nested_detach(t) for t in tensors)\n",
        "\n",
        "  return tensors.detach()\n",
        "\n",
        "def build_tokenizer():\n",
        "  \"needs model param and cache_dir\"\n",
        "  tokenizer = None\n",
        "  print(\"\\nUsing RoBERTa tokenizer\")\n",
        "  tokenizer = RobertaTokenizerFast.from_pretrained(args.model, cache_dir = args.cache_dir)\n",
        "  return tokenizer\n",
        "\n",
        "def build_model(args):\n",
        "  model = None\n",
        "  print(\"\\nUsing RoBERTa model\")\n",
        "  model = RobertaModel.from_pretrained(args.model, cache_dir = args.cache_dir)\n",
        "  return model\n",
        "\n",
        "def build_optimizer(model, args):\n",
        "  optimizer = None\n",
        "  if args.optimizer == \"adam\":\n",
        "    print(\"\\nUsing Adam\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "  elif args.optimizer == \"adamw\":\n",
        "    print(\"\\nUsing AdamW\")\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "\n",
        "  return optimizer\n",
        "\n",
        "def build_scheduler(optimizer, train_steps, args):\n",
        "    scheduler = None\n",
        "    if args.scheduler == \"linear_warmup\":\n",
        "      print(\"\\nUsing linear warmup scheduler\")\n",
        "      warmup_steps = int(args.warmup_ratio * train_steps)\n",
        "      scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, train_steps)\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "def check_data_pipe(loaders):\n",
        "  for loader in loaders:\n",
        "    for idx, batch in enumerate(loader):\n",
        "      print(\"*\"*50)\n",
        "      print(batch['text_lang'])\n",
        "      print(batch['text_inputs'][\"input_ids\"][:,:10])\n",
        "      print(batch['summary_lang'])\n",
        "      print(batch['summary_inputs'][\"input_ids\"][:,:10])\n",
        "      break\n",
        "\n",
        "def display_losses(mode, losses):\n",
        "  best_loss = np.min(np.array(losses))\n",
        "  best_loss_idx = np.argmin(np.array(losses)) + 1\n",
        "  print(\"Current {} loss is {:.4f}, best {} loss is {:.4f} achieved at iter {} / {}\".format(mode, losses[-1], mode, best_loss, best_loss_idx, len(losses)))\n",
        "\n",
        "\n",
        "def display_scores(mode, scores):\n",
        "  for k in scores.keys():\n",
        "    scores_k = scores[k]\n",
        "    if \"loss\" in k:\n",
        "      best_score_k = np.min(np.array(scores_k))\n",
        "      best_score_k_idx = np.argmin(np.array(scores_k)) + 1\n",
        "    else:\n",
        "      best_score_k = np.max(np.array(scores_k))\n",
        "      best_score_k_idx = np.argmax(np.array(scores_k)) + 1\n",
        "    print(\"Best {} {} is {:.4f} achieved at iter {} / {}\".format(mode, k, best_score_k, best_score_k_idx, len(scores_k)))\n",
        "\n",
        "\n",
        "def compute_r1s(sents):\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=False)\n",
        "\n",
        "  all_r1s = []\n",
        "  for i in range(len(sents)):\n",
        "    pruned_sents = sents[:i] + sents[(i + 1):]\n",
        "    pruned_text = \" \".join(pruned_sents)\n",
        "    scores = scorer.score(pruned_text, sents[i])\n",
        "    r1 = 100 * scores[\"rouge1\"].fmeasure\n",
        "    all_r1s.append(r1)\n",
        "  all_r1s = np.array(all_r1s)\n",
        "\n",
        "  return all_r1s\n",
        "\n",
        "\n",
        "def check_scores(dataset):\n",
        "  all_scores = []\n",
        "  for i in tqdm(range(len(dataset.scored_summaries))):\n",
        "    scores = dataset.scored_summaries[i][1]\n",
        "    max_score = np.max(np.array(scores))\n",
        "    all_scores.append(max_score)\n",
        "  m_score = np.mean(all_scores)\n",
        "\n",
        "  return m_score\n"
      ],
      "metadata": {
        "id": "ndX4Hoz0J5Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HyperParameters for my implementation\n",
        "gen_method = \"diverse_beam_search\"\n",
        "scoring_methods = [\"rouge_1\", \"rouge_2\", \"rouge_l\"]\n",
        "filter_out_duplicates = True\n",
        "sep_symbol = \"[SEP]\"\n",
        "n_beams = 15\n",
        "n_tasks = 3\n",
        "use_shared_bottom = True\n",
        "\n",
        "hidden_size = 1024\n",
        "bottom_hidden_size = 1024\n",
        "expert_hidden_size = 1024\n",
        "tower_hidden_size = 1024\n",
        "\n",
        "num_experts = 6\n",
        "k = 3 #choices for topk\n",
        "\n",
        "max_length = 512\n",
        "sharp_pos = False\n",
        "use_aux_los =False\n",
        "max_train_size = 1000000\n",
        "max_validation_size = 10000\n",
        "max_test_size = 10000\n",
        "\n",
        "train_sizes = [143000,144113]\n",
        "val_size= 13368\n",
        "test_sizes = 11490\n",
        "max_source_length = 384\n",
        "max_summary_length = 128\n",
        "eval_threshold = 500\n",
        "\n",
        "pegasus_model_names = [\"pegasus_second_half\", \"pegasus_first_half\"]"
      ],
      "metadata": {
        "id": "QDUCBWQUM6WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#demo HyperParams(uses argparser)\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(args = [])\n",
        "\n",
        "args.device = torch.device(\"cuda\")\n",
        "args.generation_methods = [\"diverse_beam_search\"]\n",
        "args.num_beams = 15\n",
        "args.scoring_methods = [\"rouge_1\", \"rouge_2\", \"rouge_l\"]\n",
        "args.filter_out_duplicates = True\n",
        "args.sep_symbol = \"[SEP]\"\n",
        "args.n_tasks = 3\n",
        "args.hidden_size = 1024\n",
        "args.use_shared_bottom = True\n",
        "args.bottom_hidden_size = 1024\n",
        "args.num_experts = 6\n",
        "args.k = 3\n",
        "args.expert_hidden_size = 1024\n",
        "args.tower_hidden_size = 1024\n",
        "args.sharp_pos = False\n",
        "args.use_aux_loss = False\n",
        "args.max_length = 512\n",
        "args.max_source_length = 384\n",
        "args.max_summary_length = 128"
      ],
      "metadata": {
        "id": "8Fu3Wpp0_Z5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "base_model_name = \"google/pegasus-cnn_dailymail\"\n",
        "base_tokenizer = PegasusTokenizer.from_pretrained(base_model_name)\n",
        "base_model = PegasusForConditionalGeneration.from_pretrained(base_model_name)\n",
        "base_model = base_model.to(args.device)\n",
        "base_model = base_model.eval()\n",
        "\n",
        "# candidates\n",
        "tok_text = base_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=1024)\n",
        "tok_text[\"input_ids\"] = tok_text[\"input_ids\"][:, :1024]\n",
        "tok_text[\"attention_mask\"] = tok_text[\"attention_mask\"][:, :1024]\n",
        "with torch.no_grad():\n",
        "    generated = base_model.generate(\n",
        "        input_ids=tok_text[\"input_ids\"].to(args.device),\n",
        "        attention_mask=tok_text[\"attention_mask\"].to(args.device),\n",
        "        num_beams=15,\n",
        "        num_beam_groups=15,\n",
        "        diversity_penalty=1.0,\n",
        "        num_return_sequences=15,\n",
        "        repetition_penalty=1.0,\n",
        "        length_penalty=0.8,\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "candidates = base_tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer = True)\n",
        "print(\"\\nSummary candidates:\")\n",
        "for j in range(len(candidates)):\n",
        "    candidates[j] = candidates[j].replace(\"<n>\", \" \")\n",
        "    candidates[j] = \"\\n\".join(sent_tokenize(candidates[j]))\n",
        "    rouge_scores = scorer.score(label, candidates[j])\n",
        "    r1 = 100 * rouge_scores[\"rouge1\"].fmeasure\n",
        "    r2 = 100 * rouge_scores[\"rouge2\"].fmeasure\n",
        "    rl = 100 * rouge_scores[\"rougeLsum\"].fmeasure\n",
        "    mean_r = (r1 + r2 + rl) / 3\n",
        "    print(\"\\nCandidates {} (Mean R: {:.2f}, R-1: {:.2f}, R-2: {:.2f}, R-L: {:.2f})\".format(j, mean_r, r1, r2, rl))\n",
        "    candidates[j] = candidates[j].replace(\"\\n\", \" \")\n",
        "    print(candidates[j])\n",
        "\n",
        "del base_model\n",
        "del tok_text\n",
        "del generated\n",
        "gc.collect()\n",
        "\n",
        "# SummaReranker\n",
        "# model\n",
        "model_name = \"roberta-large\"\n",
        "tokenizer = RobertaTokenizerFast(model_name)\n",
        "model = RobertaModel(model_name)\n",
        "model = model.to(args.device)\n",
        "summareranker_model = ModelMultitaskBinary(model, tokenizer, args)\n",
        "summareranker_model = summareranker_model.to(args.device)\n",
        "summareranker_model_path = \"/data/mathieu/2nd_stage_summarization/4_supervised_multitask_reranking/saved_models/cnndm/multitask_3_tasks_ablation_8/checkpoint-12500/pytorch_model.bin\"\n",
        "summareranker_model.load_state_dict(torch.load(summareranker_model_path))\n",
        "summareranker_model = summareranker_model.eval()\n",
        "# prepare the data\n",
        "text_inputs = tokenizer(text, return_tensors=\"pt\", max_length=args.max_source_length, padding='max_length')\n",
        "text_inputs[\"input_ids\"] = text_inputs[\"input_ids\"][:, :args.max_source_length]\n",
        "text_and_candidates_ids, text_and_candidates_masks = [], []\n",
        "for j in range(len(candidates)):\n",
        "    candidate = candidates[j]\n",
        "    candidate_inputs = tokenizer(candidate, return_tensors=\"pt\", max_length=args.max_summary_length, padding='max_length')\n",
        "    candidate_inputs[\"input_ids\"] = candidate_inputs[\"input_ids\"][:, :args.max_summary_length]\n",
        "    block = tokenizer.batch_decode(text_inputs[\"input_ids\"], skip_special_tokens = True)[0] + args.sep_symbol + tokenizer.batch_decode(candidate_inputs[\"input_ids\"], skip_special_tokens = True)[0]\n",
        "    text_and_candidate = tokenizer(block, return_tensors=\"pt\", padding=\"max_length\", max_length=args.max_length)\n",
        "    ids = text_and_candidate[\"input_ids\"][:, :args.max_length]\n",
        "    mask = text_and_candidate[\"attention_mask\"][:, :args.max_length]\n",
        "    text_and_candidates_ids.append(ids)\n",
        "    text_and_candidates_masks.append(mask)\n",
        "text_and_candidates_ids = torch.cat(text_and_candidates_ids, 0).unsqueeze(0)\n",
        "text_and_candidates_ids = text_and_candidates_ids.to(args.device)\n",
        "text_and_candidates_masks = torch.cat(text_and_candidates_masks, 0).unsqueeze(0)\n",
        "text_and_candidates_masks = text_and_candidates_masks.to(args.device)\n",
        "# inference\n",
        "mode = torch.tensor([0]).to(args.device)\n",
        "scores = torch.randn(1, len(args.scoring_methods), len(candidates))\n",
        "scores = scores.to(args.device) # create random candidate scores\n",
        "with torch.no_grad():\n",
        "    output = summareranker_model(\n",
        "        mode,\n",
        "        text_and_candidates_ids,\n",
        "        text_and_candidates_masks,\n",
        "        scores\n",
        "    )\n",
        "candidate_scores = output[\"overall_predictions\"][0]\n",
        "print(\"\\nSummaReranker predicted scores:\")\n",
        "for j in range(len(candidates)):\n",
        "    print(\"Candidate {} has score: {:.4f}\".format(j, candidate_scores[j]))\n",
        "best_idx = np.argmax(np.array(candidate_scores))\n",
        "print(\"\\nSummaReranker output summary is candidate #{}\".format(best_idx))\n",
        "print(candidates[best_idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "GPDHj-3a7LTK",
        "outputId": "54c6c44f-c242-4e0e-eb7c-0242b9a51adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-007d3a784934>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbase_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"google/pegasus-cnn_dailymail\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbase_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPegasusTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPegasusForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PegasusTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgacWgIf-AKR",
        "outputId": "cfa8e1c8-9d63-4fad-a7e5-ca936c89d9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=2aac40109fbc4ca64f9e63fddbc503649bdbc14faea993a26de95dd112ff5f8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IN WORK, don't run after preceeding block\n",
        "#Dataset __init__\n",
        "\n",
        "save_path = data_pretrained + \"pretrained/\"\n",
        "#device choice\n",
        "device = torch.device(\"cpu\")\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "  print(\"Using device: {}\".format(device))\n",
        "\n",
        "#tokenizer\n",
        "  tokenizer = build_tokenizer(\"roberta-large\")\n",
        "\n",
        "#datasets\n",
        "for x in [(train_datasets, train_sizes), (val_dataset, val_size), (test_dataset, test_size)]:\n",
        "\n",
        "  set, size = x\n",
        "  texts, summaries, scored_summaries = load_data(set, size, args, individual_txt=args.highlights, train=train)\n",
        "  print(\"loaded new data!\", len(texts), len(summaries), len(scored_summaries), len(scored_summaries[0]),\n",
        "        len(scored_summaries[0][0]), len(scored_summaries[0][1]), len(scored_summaries[0][1][0]))\n",
        "\n",
        "  mode = \"train\"\n",
        "  if not (train):\n",
        "      mode = \"val\"\n",
        "  dataset = MultitaskRerankingDatasetTrain(mode, tokenizer, texts, scored_summaries, summaries, args)\n",
        "  datasets.append(dataset)\n",
        "  print(\"There are {} {} batches\".format(int(len(dataset.texts) / args.train_bs), set))\n",
        "\n",
        "train_dataset = datasets[0]\n",
        "train_dataset.texts = train_dataset.texts[:args.max_train_size]\n",
        "train_dataset.scored_summaries = train_dataset.scored_summaries[:args.max_train_size]\n",
        "train_dataset.labels = train_dataset.labels[:args.max_train_size]\n",
        "\n",
        "val_dataset = datasets[1]\n",
        "val_dataset.texts = val_dataset.texts[:args.max_val_size]\n",
        "val_dataset.scored_summaries = val_dataset.scored_summaries[:args.max_val_size]\n",
        "val_dataset.labels = val_dataset.labels[:args.max_val_size]\n",
        "\n",
        "test_dataset = datasets[2]\n",
        "test_dataset.texts = test_dataset.texts[:args.max_test_size]\n",
        "test_dataset.scored_summaries = test_dataset.scored_summaries[:args.max_test_size]\n",
        "test_dataset.labels = test_dataset.labels[:args.max_test_size]\n",
        "\n",
        "print(train_dataset.texts[0])\n",
        "  print(\"*\" * 30)\n",
        "  print(val_dataset.texts[0])\n",
        "  print(\"*\" * 30)\n",
        "  print(test_dataset.texts[0])"
      ],
      "metadata": {
        "id": "JDKXQpFRyYWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check oracle\n",
        "m_train_score = check_scores(train_dataset)\n",
        "m_val_score = check_scores(val_dataset)\n",
        "m_test_score = check_scores(test_dataset)\n",
        "print(\"\\nOracle - train: {:.4f}, val: {:.4f}, test: {:.4f}\".format(m_train_score, m_val_score, m_test_score))\n"
      ],
      "metadata": {
        "id": "iPeGr2li3Xq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model __init__\n",
        "pretrained_model = build_model(args)\n",
        "model = ModelMultitaskBinary(pretrained_model, tokenizer, args)\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"\\nThe model has {} trainable parameters\".format(n_params))\n",
        "model = model.to(device)\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "        output_dir=args.save_model_path,  # will be changed\n",
        "        overwrite_output_dir=True,\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        do_predict=False,\n",
        "        evaluation_strategy=args.evaluation_strategy,\n",
        "        eval_steps=args.eval_every,\n",
        "        save_total_limit=args.n_checkpoints_to_save,\n",
        "        num_train_epochs=args.n_epochs,\n",
        "        adafactor=args.adafactor,\n",
        "        lr_scheduler_type=args.scheduler,\n",
        "        warmup_ratio=args.warmup_ratio,\n",
        "        per_device_train_batch_size=args.train_bs,\n",
        "        per_device_eval_batch_size=args.inference_bs,\n",
        "        learning_rate=args.lr,\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        weight_decay=args.wd,\n",
        "        max_grad_norm=args.gradient_clipping,\n",
        "        logging_strategy=\"no\",\n",
        "        save_strategy=args.evaluation_strategy,\n",
        "        save_steps=args.eval_every,\n",
        "        metric_for_best_model=args.metric_for_best_model,\n",
        "        fp16=args.fp16,\n",
        "        load_best_model_at_end=True,\n",
        "        greater_is_better=True,\n",
        "        disable_tqdm=False,\n",
        "        deepspeed=args.deepspeed,\n",
        "        sharded_ddp=args.sharded_ddp,\n",
        "        local_rank=args.local_rank\n",
        "    )\n",
        "data_collator = default_data_collator\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=train_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n"
      ],
      "metadata": {
        "id": "T-fbJrxA3bUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "if args.eval_epoch_0:\n",
        "  results = trainer.evaluate()\n",
        "  print(\"*\" * 50, \"Init VAL results:\")\n",
        "  print(results)\n",
        "  model.moe.display_tasks_probs()\n",
        "\n",
        "if args.train:\n",
        "  trainer.train()\n",
        "else:\n",
        "  if args.load_model:\n",
        "    model.load_state_dict(torch.load(args.load_model_path))\n",
        "    print(\"Loaded the model weights!\", args.load_model_path)\n",
        "\n",
        "# validate with the best model\n",
        "results = trainer.evaluate()\n",
        "print(\"\\n\", \"*\" * 50, \"BEST VAL RESULTS\")\n",
        "print(results)\n",
        "model.moe.display_tasks_probs()\n",
        "\n",
        "# test results\n",
        "test_results = trainer.predict(test_dataset)\n",
        "print(\"\\n\", \"*\" * 50, \"TEST RESULTS:\")\n",
        "print(test_results[2])\n",
        "model.moe.display_tasks_probs()"
      ],
      "metadata": {
        "id": "1R0G4xX23pEG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}